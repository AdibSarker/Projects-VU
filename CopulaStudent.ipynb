{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de cel hier onder te runnen moet er een package geinstalleerd worden via anaconda prompt. Deze package zorgt ervoor dat we functies uit andere scripts kunnen gerbuiken in dit script.\n",
    "<break>\n",
    "Tik het volgende in anaconda prompt:\n",
    "<br \\>\n",
    "**pip install ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "from pandas_datareader import data as wb\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.Dataset import getdata\n",
    "from ipynb.fs.full.Dataset import getreturns\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as st\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hier onder wordt de dataset verkregen. Ik gebruik 2010 tot en met 2015 als mijn in-sample data.\n",
    "<break>\n",
    "Hiervoor gebruik een functie die in een ander script (Dataset) is geschreven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getreturns()\n",
    "mInSampleReturns = df.loc[:\"2015\"]\n",
    "mOutSampleReturns = df.loc[\"2016\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier schat ik de garch modellen met student-t verdeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Parameters_GARCH_Student(vReturns):\n",
    "    dOmega = 0.1\n",
    "    dAlpha = 0.1\n",
    "    dBeta = 0.8\n",
    "    dNu = 4.2\n",
    "    vTheta = np.array([dOmega, dAlpha, dBeta, dNu])\n",
    "    \n",
    "    def LL_GARCH_Student(vTheta,returns):\n",
    "        dOmega = vTheta[0]\n",
    "        dAlpha = vTheta[1]\n",
    "        dBeta  = vTheta[2]\n",
    "        dNu = vTheta[3]\n",
    "        iT=len(returns)\n",
    "        vH=np.zeros(iT)\n",
    "        \n",
    "        for t in range(iT):\n",
    "            if t == 0:\n",
    "                vH[t] = np.var(returns) \n",
    "            else:\n",
    "                vH[t] = dOmega + dAlpha*returns[t-1]**2 + dBeta * vH[t-1]    \n",
    "        \n",
    "        vLogPdf = gammaln( (dNu+1)/2 )-gammaln( dNu/2 ) - 0.5*np.log( np.pi*(dNu-2)*vH[1:] ) \\\n",
    "                - 0.5 * (dNu+1) * np.log( 1 + ( vReturns[1:]**2 / ( (dNu-2)*vH[1:] ) ) )\n",
    "        return -np.sum(vLogPdf)\n",
    "    \n",
    "    def Optimizer(returns, initials, function, bnds):\n",
    "        result = minimize(function, initials, args=(returns), \\\n",
    "                          options ={'eps':1e-09, 'disp': True, 'maxiter':200}, method='SLSQP',bounds=bnds)\n",
    "        return result\n",
    "    bounds = ((0, 1), (0, 1), (0, 1), (2.1,50))\n",
    "    result=Optimizer(vReturns, vTheta, LL_GARCH_Student, bounds)\n",
    "    return result.x, -result.fun, result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2492.9007501403294\n",
      "            Iterations: 16\n",
      "            Function evaluations: 107\n",
      "            Gradient evaluations: 16\n",
      "[ 0.04129691  0.09040065  0.89042978  6.9246734 ]\n",
      "-2492.9007501403294\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(parameter1_Stud,likelihood1_Stud,success1_Stud)=Parameters_GARCH_Student(np.array(mInSampleReturns.iloc[:,0]))\n",
    "print(parameter1_Stud)\n",
    "print(likelihood1_Stud)\n",
    "print(success1_Stud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2217.307544488237\n",
      "            Iterations: 16\n",
      "            Function evaluations: 108\n",
      "            Gradient evaluations: 16\n",
      "[ 0.02623554  0.09365917  0.89023515  6.98573207]\n",
      "-2217.307544488237\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(parameter2_Stud,likelihood2_Stud,success2_Stud)=Parameters_GARCH_Student(np.array(mInSampleReturns.iloc[:,1]))\n",
    "print(parameter2_Stud)\n",
    "print(likelihood2_Stud)\n",
    "print(success2_Stud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeSigma2GARCH(vTheta1,vTheta2,mReturns):\n",
    "    iT = mReturns.shape[1]\n",
    "    iDimension = mReturns.shape[0]\n",
    "    mH = np.zeros((iDimension,iT))\n",
    "    dOmega1 = vTheta1[0]\n",
    "    dOmega2 = vTheta2[0]\n",
    "    dAlpha1 = vTheta1[1]\n",
    "    dAlpha2 = vTheta2[1]\n",
    "    dBeta1 = vTheta1[2]\n",
    "    dBeta2 = vTheta2[2]\n",
    "    for t in range(iT):\n",
    "        if t==0:\n",
    "            mH[0,t]=np.var(mReturns[0,:])\n",
    "            mH[1,t]=np.var(mReturns[1,:])\n",
    "        else:\n",
    "            mH[0,t]=dOmega1 + dAlpha1 * mReturns[0,t-1]**2 + dBeta1 * mH[0,t-1]\n",
    "            mH[1,t]=dOmega2 + dAlpha2 * mReturns[1,t-1]**2 + dBeta2 * mH[1,t-1]\n",
    "    return mH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier schat ik de waardes van de sigma's over tijd voor alle 2 de tijdreeksen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mSigma2=ComputeSigma2GARCH(parameter1_Stud,parameter2_Stud,np.array(mInSampleReturns).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier voer ik de PIT. De marginal density is student-t verdeeld met mean=0 and variance=$\\sigma_t^2$\n",
    "Met de functie st.t.cdf zorg ik ervoor dat de marginal cdf van elke $y_t$ wordt berekend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00083545  0.00097807]\n",
      "[ 0.9999183   0.99973922]\n"
     ]
    }
   ],
   "source": [
    "mReturns=np.array(mInSampleReturns).T\n",
    "mScale=np.sqrt(mSigma2)\n",
    "\n",
    "vUt1=st.t.cdf(mReturns[0,:], parameter1_Stud[3], loc=np.zeros(mReturns.shape[1]), \\\n",
    "              scale=np.sqrt((parameter1_Stud[3]-2)/parameter1_Stud[3]*mScale[0,:]))\n",
    "vUt2=st.t.cdf(mReturns[1,:], parameter2_Stud[3], loc=np.zeros(mReturns.shape[1]), \\\n",
    "              scale=np.sqrt((parameter2_Stud[3]-2)/parameter2_Stud[3]*mScale[1,:]))\n",
    "vUt1=vUt1.reshape((1,len(mInSampleReturns)))\n",
    "vUt2=vUt2.reshape((1,len(mInSampleReturns)))\n",
    "mUt=np.concatenate((vUt1, vUt2), axis=0)\n",
    "print(np.min(mUt,axis=1))\n",
    "print(np.max(mUt,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copula part\n",
    "\n",
    "Vanaf hieronder zal ik de copula LL gaan uitwerken. Eerst moet ik de PIT transformatie toepassen op de returns. Hierbij moet ik de kans berekenen dat:\n",
    "$$ P(Y_t\\leq y_t)=u_t \\quad Y_t\\sim \\text{t}(0,\\sigma_t^2, \\nu) $$\n",
    "Waarbij $Y_t$ een stochast is en $y_t$ de realisatie.\n",
    "\n",
    "Met de $u_t$ gaan we de eerste inverse cdf toepassen op de marginale copula density met mean 0 and std=1. In andere woorden we willen:\n",
    "$x_{it}=t^{-1}_\\nu(u_{it})$\n",
    "\n",
    "De marginale copula density met student-t distribution ziet er als volgt uit:\n",
    "$$ t_\\nu(x)=\\frac{\\Gamma\\left(\\frac{\\nu_c+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_c}{2}\\right)\\nu^{1/2}\\pi^{1/2}}\\left(1+\\frac{x^2}{\\nu_c}\\right)^{-\\frac{\\nu_c+1}{2}}$$\n",
    "Dit is de standaard student-t verdeling. Ik maak gebruik van de functie st.ppf() om de inverse cdf van de bijbehorende $u_t$ te bepalen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De log-likelihood die hierbij gemaximaliseerd word is als volgt gedefinieerd:\n",
    "\n",
    "$$\\log\\left(\\Gamma\\left(\\frac{\\nu+d}{2}\\right)\\right)+(d-1)\\log\\left(\\Gamma\\left(\\frac{\\nu}{2}\\right)\\right)-d\\log\\left(\\Gamma\\left(\\frac{\\nu+1}{2}\\right)\\right)-\\frac{1}{2}\\log|R| - \\frac{\\nu+d}{2}\\log\\left(1+\\frac{x'R^{-1}x}{\\nu}\\right)+\\frac{\\nu+1}{2}\\log\\left(1+\\frac{x_1^2}{\\nu}\\right)+\\frac{\\nu+1}{2}\\log\\left(1+\\frac{x_2^2}{\\nu}\\right)$$\n",
    "\n",
    "- x is gedefinieerd als:\n",
    "$$x=\\begin{pmatrix} x_{1}\\\\ x_{2}\\end{pmatrix}$$\n",
    "\n",
    "- d is de dimensie. In dit geval gelijk aan 2\n",
    "\n",
    "- $R$ is als volgt gedefinieerd:\n",
    "\n",
    "$$R=\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$$\n",
    "<break>\n",
    "De parameters die we optimaliseren zijn $\\rho,\\nu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Parameters_Copula_Student(mReturns,mUt):\n",
    "    dRho12 = 0.2\n",
    "    dNu = 4.1\n",
    "    vTheta = np.array([dRho12, dNu])\n",
    "    \n",
    "    def LL_Copula_Student(vTheta,mReturns,mUt):\n",
    "        dRho12 = vTheta[0]\n",
    "        dNu = vTheta[1]\n",
    "        iT = mReturns.shape[1]\n",
    "        iDimension = len(mReturns)\n",
    "        mQuantile= st.t.ppf(mUt, dNu, loc=np.zeros(mUt.shape), scale=np.ones(mUt.shape))\n",
    "        mR = np.ones((iDimension,iDimension))\n",
    "        mR[1,0] = dRho12\n",
    "        mR[0,1] = dRho12\n",
    "        \n",
    "        dSum=0\n",
    "        \n",
    "        for t in range(1,iT):\n",
    "            vX = mQuantile[:,t]\n",
    "            vX = vX.reshape((iDimension,1))\n",
    "            dLogf1=st.t.logpdf(vX[0],dNu)\n",
    "            dLogf2=st.t.logpdf(vX[1],dNu)\n",
    "#             dLogLikelihood = gammaln((dNu+2)/2) - gammaln(dNu/2) - 0.5*np.log((np.pi*dNu)**2) - 0.5 * np.log(det(mR)) \\\n",
    "#                             - (dNu+2)/2 * np.log(1+(vX.T @ inv(mR) @ vX)/dNu) - dLogf1-dLogf2\n",
    "            dLogLikelihood = gammaln((dNu+2)/2) + gammaln(dNu/2) - iDimension * gammaln((dNu+1)/2) - 1/2 * np.log(det(mR)) \\\n",
    "                             -(dNu+iDimension)/2 *np.log(1+(vX.T@inv(mR)@vX)/dNu) + (dNu+1)/2 * np.log(1+vX[0]**2/dNu) + \\\n",
    "                             (dNu+1)/2* np.log(1+vX[1]**2/dNu)             \n",
    "                \n",
    "            dSum += np.asscalar(dLogLikelihood)\n",
    "    \n",
    "        return -dSum\n",
    "    \n",
    "    def Optimizer(returns, mUt, initials, function, bnds):\n",
    "        result = minimize(function, initials, args=(returns,mUt), \\\n",
    "                          options ={'eps':1e-09, 'disp': True, 'maxiter':200}, method='SLSQP',bounds=bnds)\n",
    "        return result\n",
    "    bounds = ((-0.9999999, 0.9999999),(2.1,50))\n",
    "    result=Optimizer(mReturns, mUt, vTheta, LL_Copula_Student, bounds)\n",
    "    return result.x, -result.fun, result.success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dLogLikelihood = gammaln((dNu+2)/2) + gammaln(dNu/2) - iDimension*gammaln((dNu+1)/2) - (dNu+iDimension)/2 * \\\n",
    "                 np.log(1+(vX.T@inv(mR)@vX)/dNu) + (dNu+1)/2*np.log(1+vX[0]**2/dNu) + \\\n",
    "                 (dNu+1)/2*np.log(1+vX[1]**2/dNu) \n",
    "dSum+=np.asscalar(dLogLikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -1531.9034059941157\n",
      "            Iterations: 33\n",
      "            Function evaluations: 151\n",
      "            Gradient evaluations: 30\n",
      "[ 0.91677754  7.40781997]\n",
      "1531.9034059941157\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(vTheta,LL_Copula_St,success_Copula_St)=Parameters_Copula_Student(np.array(mInSampleReturns).T,mUt)\n",
    "print(vTheta)\n",
    "print(LL_Copula_St)\n",
    "print(success_Copula_St)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AIC(vTheta1, vTheta2, vTheta3, dLL1, dLL2, dLL3):\n",
    "    iK=len(vTheta1)+len(vTheta2)+len(vTheta3)\n",
    "    dLL=dLL1+dLL2+dLL3\n",
    "    dAIC= 2*iK-2*dLL\n",
    "    print(dAIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6376.609777268901\n"
     ]
    }
   ],
   "source": [
    "AIC(parameter1_Stud,parameter2_Stud,vTheta, likelihood1_Stud, likelihood2_Stud , LL_Copula_St)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LL_GARCH_Student(vTheta,vReturns):\n",
    "    dOmega = vTheta[0]\n",
    "    dAlpha = vTheta[1]\n",
    "    dBeta  = vTheta[2]\n",
    "    dNu = vTheta[3]\n",
    "    iT=len(vReturns)\n",
    "    vH=np.zeros(iT)\n",
    "\n",
    "    for t in range(iT):\n",
    "        if t == 0:\n",
    "            vH[t] = np.var(vReturns) \n",
    "        else:\n",
    "            vH[t] = dOmega + dAlpha*vReturns[t-1]**2 + dBeta * vH[t-1]    \n",
    "\n",
    "    vLogPdf = gammaln( (dNu+1)/2 )-gammaln( dNu/2 ) - 0.5*np.log( np.pi*(dNu-2)*vH[1:] ) \\\n",
    "            - 0.5 * (dNu+1) * np.log( 1 + ( vReturns[1:]**2 / ( (dNu-2)*vH[1:] ) ) )\n",
    "    return np.sum(vLogPdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def LL_Copula_Student(vTheta,mReturns,mUt):\n",
    "        dRho12 = vTheta[0]\n",
    "        dNu = vTheta[1]\n",
    "        iT = mReturns.shape[1]\n",
    "        iDimension = len(mReturns)\n",
    "        mQuantile= st.t.ppf(mUt, dNu, loc=np.zeros(mUt.shape), scale=np.ones(mUt.shape))\n",
    "        mR = np.ones((iDimension,iDimension))\n",
    "        mR[1,0] = dRho12\n",
    "        mR[0,1] = dRho12\n",
    "        \n",
    "        dSum=0\n",
    "        \n",
    "        for t in range(1,iT):\n",
    "            vX = mQuantile[:,t]\n",
    "            vX = vX.reshape((iDimension,1))\n",
    "            dLogf1=st.t.logpdf(vX[0],dNu)\n",
    "            dLogf2=st.t.logpdf(vX[1],dNu)\n",
    "#             dLogLikelihood = gammaln((dNu+2)/2) - gammaln(dNu/2) - 0.5*np.log((np.pi*dNu)**2) - 0.5 * np.log(det(mR)) \\\n",
    "#                             - (dNu+2)/2 * np.log(1+(vX.T @ inv(mR) @ vX)/dNu) - dLogf1-dLogf2\n",
    "            dLogLikelihood = gammaln((dNu+2)/2) + gammaln(dNu/2) - iDimension * gammaln((dNu+1)/2) - 1/2 * np.log(det(mR)) \\\n",
    "                             -(dNu+iDimension)/2 *np.log(1+(vX.T@inv(mR)@vX)/dNu) + (dNu+1)/2 * np.log(1+vX[0]**2/dNu) + \\\n",
    "                             (dNu+1)/2* np.log(1+vX[1]**2/dNu)             \n",
    "                \n",
    "            dSum += np.asscalar(dLogLikelihood)\n",
    "    \n",
    "        return dSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogarithmicScore(vTheta1,vTheta2,vTheta3,mOutSampleReturns):\n",
    "    dLL1 = LL_GARCH_Student(vTheta1,np.array(mOutSampleReturns.iloc[:,0]))\n",
    "    dLL2 = LL_GARCH_Student(vTheta2,np.array(mOutSampleReturns.iloc[:,1]))\n",
    "    mReturns=np.array(mOutSampleReturns).T\n",
    "    mVarianceOutSample = ComputeSigma2GARCH(vTheta1,vTheta2,mReturns)\n",
    "    vUt1OutSample=st.t.cdf(mReturns[0,:], vTheta1[3], loc=np.zeros(mReturns.shape[1]), \\\n",
    "              scale=np.sqrt((vTheta1[3]-2)/vTheta1[3]*mVarianceOutSample[0,:]))\n",
    "    vUt2OutSample=st.t.cdf(mReturns[1,:], vTheta2[3], loc=np.zeros(mReturns.shape[1]), \\\n",
    "              scale=np.sqrt((vTheta2[3]-2)/vTheta2[3]*mVarianceOutSample[1,:]))\n",
    "    vUt1OutSample=vUt1OutSample.reshape((1,len(mOutSampleReturns)))\n",
    "    vUt2OutSample=vUt2OutSample.reshape((1,len(mOutSampleReturns)))\n",
    "    mUtOutSample=np.concatenate((vUt1OutSample, vUt2OutSample), axis=0)\n",
    "    dLL3 = LL_Copula_Student(vTheta3,mReturns,mUtOutSample)\n",
    "    dLogScore = dLL1+dLL2+dLL3\n",
    "    \n",
    "    return dLogScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLogScore = LogarithmicScore(parameter1_Stud,parameter2_Stud,vTheta,mOutSampleReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1554.67180199\n"
     ]
    }
   ],
   "source": [
    "print(dLogScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
