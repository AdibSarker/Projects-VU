{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de cel hier onder te runnen moet er een package geinstalleerd worden via anaconda prompt. Deze package zorgt ervoor dat we functies uit andere scripts kunnen gerbuiken in dit script.\n",
    "<break>\n",
    "Tik het volgende in anaconda prompt:\n",
    "<br \\>\n",
    "**pip install ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "from pandas_datareader import data as wb\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.Dataset import getdata\n",
    "from ipynb.fs.full.Dataset import getreturns\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as st\n",
    "from scipy.special import gammaln\n",
    "#import scipy\n",
    "from scipy.special import gamma\n",
    "from scipy import integrate\n",
    "from scipy.special import kv as kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hier onder wordt de dataset verkregen. Ik gebruik 2010 tot en met 2015 als mijn in-sample data.\n",
    "<break>\n",
    "Hiervoor gebruik een functie die in een ander script (Dataset) is geschreven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getreturns()\n",
    "mInSampleReturns = df.loc[:\"2015\"]\n",
    "iT=len(mInSampleReturns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier schat ik de garch modellen met skewed student-t verdeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Parameters_GARCH_Skew(vReturns):\n",
    "    dOmega_ini=0.1\n",
    "    dAlpha_ini=0.1\n",
    "    dBeta_ini=0.8\n",
    "    dNu_ini=6\n",
    "    dXi=1\n",
    "    \n",
    "    vTheta=np.array([dOmega_ini, dAlpha_ini, dBeta_ini, dNu_ini, dXi])\n",
    "    def Log_likelihood_GARCH_Skew(vTheta,vReturns):\n",
    "        iT=len(vReturns)\n",
    "        dOmega=vTheta[0]\n",
    "        dAlpha=vTheta[1]\n",
    "        dBeta=vTheta[2]\n",
    "        dNu=vTheta[3]\n",
    "        dXi=vTheta[4]\n",
    "        vH=np.zeros(iT)\n",
    "        \n",
    "        for t in range(iT):\n",
    "            if t == 0:\n",
    "                vH[t] = np.var(vReturns) \n",
    "            else:\n",
    "                vH[t] = dOmega + dAlpha*vReturns[t-1]**2 + dBeta * vH[t-1] \n",
    "        \n",
    "        dM=gamma( (dNu-1)/2 ) / ( gamma(dNu/2) )*np.sqrt( (dNu-2)/np.pi )*( dXi-1/dXi )\n",
    "        dS=np.sqrt((dXi**2+(1/dXi**2)-1)-dM**2)\n",
    "        vI = (dS * (vReturns/np.sqrt(vH))) + dM\n",
    "        for i in range(len(vI)):\n",
    "            if vI[i]<0:\n",
    "                vI[i]=-1\n",
    "            else:\n",
    "                vI[i]=1\n",
    "                      \n",
    "        vLogPdf = gammaln( (dNu+1)/2 ) - gammaln( dNu/2 ) - 1/2*np.log( ( dNu-2 ) *np.pi ) -1/2 * np.log(vH) \\\n",
    "              + np.log( dS ) + np.log( 2 / ( dXi+1/dXi ) ) - ( ( dNu+1 )/2 )*np.log( 1 + ( (dS * ( ( vReturns/np.sqrt( vH ) ) ) + dM)**2/(dNu-2))*dXi**(-2*vI)) \n",
    "        \n",
    "        \n",
    "        return -np.sum(vLogPdf)\n",
    "    \n",
    "    def Optimizer(vReturns, initials, function, bnds):\n",
    "        result = minimize(function, initials, args=(vReturns), \\\n",
    "                          options ={'eps':1e-09, 'disp': True, 'maxiter':200}, method='SLSQP',bounds=bnds)\n",
    "        return result\n",
    "    bounds = ((0, 1), (0, 1), (0, 1), (2.1, 50), (0.00001,100))\n",
    "    result=Optimizer(vReturns, vTheta, Log_likelihood_GARCH_Skew, bounds)\n",
    "    return result.x, -result.fun, result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2489.657289021924\n",
      "            Iterations: 16\n",
      "            Function evaluations: 129\n",
      "            Gradient evaluations: 16\n",
      "[ 0.03851373  0.0887035   0.89393724  6.87100222  0.90919401]\n",
      "-2489.657289021924\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(parameter1_Skew,likelihood1_Skew,success1_Skew)=Parameters_GARCH_Skew(np.array(mInSampleReturns.iloc[:,0]))\n",
    "print(parameter1_Skew)\n",
    "print(likelihood1_Skew)\n",
    "print(success1_Skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2213.559393308958\n",
      "            Iterations: 15\n",
      "            Function evaluations: 117\n",
      "            Gradient evaluations: 15\n",
      "[ 0.0234867   0.09196745  0.89452381  7.07932222  0.90333946]\n",
      "-2213.559393308958\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(parameter2_Skew,likelihood2_Skew,success2_Skew)=Parameters_GARCH_Skew(np.array(mInSampleReturns.iloc[:,1]))\n",
    "print(parameter2_Skew)\n",
    "print(likelihood2_Skew)\n",
    "print(success2_Skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier schat ik de waardes van de sigma's over tijd voor alle 2 de tijdreeksen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeSigma2GARCH(vTheta1,vTheta2,mReturns):\n",
    "    iT = mReturns.shape[1]\n",
    "    iDimension = mReturns.shape[0]\n",
    "    mH = np.zeros((iDimension,iT))\n",
    "    dOmega1 = vTheta1[0]\n",
    "    dOmega2 = vTheta2[0]\n",
    "    dAlpha1 = vTheta1[1]\n",
    "    dAlpha2 = vTheta2[1]\n",
    "    dBeta1 = vTheta1[2]\n",
    "    dBeta2 = vTheta2[2]\n",
    "    for t in range(iT):\n",
    "        if t==0:\n",
    "            mH[0,t]=np.var(mReturns[0,:])\n",
    "            mH[1,t]=np.var(mReturns[1,:])\n",
    "        else:\n",
    "            mH[0,t]=dOmega1 + dAlpha1 * mReturns[0,t-1]**2 + dBeta1 * mH[0,t-1]\n",
    "            mH[1,t]=dOmega2 + dAlpha2 * mReturns[1,t-1]**2 + dBeta2 * mH[1,t-1]\n",
    "    return mH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mSigma2=ComputeSigma2GARCH(parameter1_Skew,parameter2_Skew,np.array(mInSampleReturns).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copula part\n",
    "\n",
    "Vanaf hieronder zal ik de copula LL gaan uitwerken. Eerst moet ik de PIT transformatie toepassen op de returns. Hierbij moet ik de kans berekenen dat:\n",
    "$$ P(Y_t<y_t)=u_t \\quad Y_t\\sim \\text{Skew}(0,\\sigma_t^2, \\nu, \\xi) $$\n",
    "Waarbij $Y_t$ een stochast is en $y_t$ de realisatie.\n",
    "Er is geen ingebouwde functie voor de cdf van generalized skewed student-t verdeling. Dus moet dat eerst gehardcode worden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabability integral transformation (PIT)\n",
    "Hier bereken ik de pdf waarde van alle gridpunten met de variance van een bepaald tijdstip t. Wat ik in deze functie doe is de verdeling van elk tijdstip t op te stellen met de bijbehorende variance op tijdstip t. Als de pdf is bepaald dan bereken ik de cdf (die noem ik vCdf). Dit doe ik aan de hand van de functie scipy.integrate.cumtrapz.\n",
    "<break>\n",
    "In regel 20 heb ik dan de cdf van $Y_t$. In regel 24/25 bereken ik de integraal van de gehele pdf. Uit de integraal moet er 1 uitkomen. In regel 35 check ik of de integraal van mijn gehele pdf gelijk is aan 1. Want als dat niet zo is dan weet ik dat mijn cdf niet betrouwbaar zijn. Als het goed is elke integraal heel dichtbij 1 en in ieder geval kleiner dan 1.\n",
    "<break>\n",
    "Uiteindelijk wil ik voor de PIT de cdf weten die hoort bij $ P(Y_t<y_t)$. Dit doe ik dan aan de hand van dummy variables. Als de waarde van mijn gridpoint op positie i $\\leq y_t$ dan krijgt dat punt een waarde 1 en anders een 0. Ik tel dan de som van de dummy variables. Dan weet ik hoeveel gridpoints een waarde 1 hebben. Uiteindelijk doe ik -1 zodat ik de positie heb van de laatste gridpoint die een waarde heeft van 1. Uitendelijk return ik de bijbehorende cdf \n",
    "<break>\n",
    "Dit proces voor ik voor elke $y_t$ uit met de bijbehorende parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdfSkew(vTheta, dReturn, dVariance):\n",
    "    vGridPoints=np.arange(-20,20,0.001)\n",
    "    dOmega=vTheta[0]\n",
    "    dAlpha=vTheta[1]\n",
    "    dBeta=vTheta[2]\n",
    "    dNu = vTheta[3]\n",
    "    dXi = vTheta[4]\n",
    "    dM=gamma( (dNu-1)/2 ) / ( gamma(dNu/2) )*np.sqrt( (dNu-2)/np.pi )*( dXi-1/dXi )\n",
    "    dS=np.sqrt((dXi**2+(1/dXi**2)-1)-dM**2)\n",
    "    vCondition = (dS * (vGridPoints/np.sqrt(dVariance))) + dM\n",
    "    vI1 = vCondition>=0\n",
    "    vI1 = vI1 + np.zeros(len(vCondition))\n",
    "    vI2 = vCondition<0\n",
    "    vI2 = -1*(vI2 + np.zeros(len(vCondition)))\n",
    "    vI  = vI1+vI2    \n",
    "   \n",
    "    vPdf = gamma((dNu+1)/2)/gamma(dNu/2) * ((dNu-2)*np.pi*dVariance)**(-1/2) * dS * ( 2 / (dXi+1/dXi) ) * \\\n",
    "           (1 + ( (dS * ( ( vGridPoints/np.sqrt( dVariance ) ) ) + dM)**2/(dNu-2))*dXi**(-2*vI))**(-(dNu+1)/2)\n",
    "    \n",
    "    vCdf = integrate.cumtrapz(x=vGridPoints, y=vPdf, initial = 0)\n",
    "   \n",
    "    dProbability=integrate.simps(x=vGridPoints, y = vPdf)\n",
    "    \n",
    "    if dProbability<0.99 or dProbability>1 :\n",
    "        print(\"does not integrate to 1\")\n",
    "    \n",
    "    vDummy=vGridPoints<dReturn\n",
    "    vDummy=vDummy+np.zeros(len(vCdf))\n",
    "    dIndex=np.sum(vDummy)-1\n",
    "    return vCdf[int(dIndex)]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de cel hieronder verkrijg ik de probabilities van de PIT. Ik zal later de inverse nemen van de probabilities, zodat ik de kwantielen later in de copula log-likelihood kan stoppen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vU1=np.zeros((iT,1))\n",
    "vU2=np.zeros((iT,1))\n",
    "for i in range(iT):\n",
    "    vU1[i]=cdfSkew(parameter1_Skew,np.array(mInSampleReturns)[i,0],mSigma2[0,i])\n",
    "    vU2[i]=cdfSkew(parameter2_Skew,np.array(mInSampleReturns)[i,1],mSigma2[1,i])   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00153362761199\n",
      "0.999464619734\n",
      "0.00187266972411\n",
      "0.999012528518\n"
     ]
    }
   ],
   "source": [
    "print(np.min(vU1))\n",
    "print(np.max(vU1))\n",
    "print(np.min(vU2))\n",
    "print(np.max(vU2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berekenen van de CDF\n",
    "Om de kwantielen te bepalen moeten we eerst de CDF opstellen van ons univariate copula met skewed student-t verdeling. De verdeling is voor elk tijdstip hetzelfde. Vandaar dat we eerst de CDF opstellen en die voor elk tijdstip t gaan gebruiken om de kwantielen te bepalen. De pdf met mean=0 en variance=1 van de univariate copula met skewed student-t verdeling ziet er als volgt uit:\n",
    "\n",
    "$$f(x|\\nu,\\xi_i)=\\frac{c\\mathbb{K}_{\\frac{1+\\nu}{2}}\\left(\\sqrt{(\\nu+x^2)\\xi_i^2}\\right)\\exp(x\\xi_i)}{\\left(\\sqrt{(\\nu+x^2)\\xi_i^2}\\right)^{-\\frac{1+\\nu}{2}}\\left(1+\\frac{x^2}{\\nu}\\right)^\\frac{1+\\nu}{2}}$$\n",
    "\n",
    "- c is als volgt gedefinieerd:\n",
    "$$c=\\frac{2^{\\frac{2-(\\nu+1)}{2}}}{\\Gamma\\left(\\frac{\\nu}{2}\\right)(\\pi\\nu)^{\\frac{1}{2}}}$$\n",
    "\n",
    "- $\\mathbb{K}_{\\frac{1+\\nu}{2}}(.)$ is de modified Bessel function of second kind\n",
    "<break>\n",
    "\n",
    "Ik maak weer een grid aan van -20 tot 20 en op elke grid bereken ik de bijbehorende pdf waarde. Uiteindelijk maak ik gebruik van de functie integrate.cumtrapz om van pdf naar cdf te gaan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetCDF(dNu,dXi):\n",
    "    vGridPoints = np.arange(-20,20,0.001)\n",
    "    iDimension = 1\n",
    "    dC = 2**( ( 2- ( dNu+iDimension ) ) / 2) / (gamma(dNu/2) * np.sqrt(np.pi*dNu) )\n",
    "    dSuperscript = np.sqrt((dNu + vGridPoints**2)*dXi**2)\n",
    "    vPdf = (dC * kv((dNu+iDimension)/2,dSuperscript) * np.exp(vGridPoints*dXi)) / (dSuperscript**(- ( iDimension+dNu )/2 ) \\\n",
    "            * (1+ vGridPoints**2/dNu)**((iDimension+dNu)/2))\n",
    "\n",
    "    vCdf = integrate.cumtrapz(x=vGridPoints, y=vPdf, initial = 0)\n",
    "    return vCdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Het verkrijgen van de kwantielen.\n",
    "\n",
    "In de copula log-likelihood moeten we de kwantielen van de PIT gebruiken. Hiervoor moeten we de inverse CDF gebruiken van de univariate copula skewed student-t met mean=0 and variance=1. In de functie hieronder roep ik de cdf van de univariate copula skewed student-t verdeling op met mean 0 and std 0. Aan de hand van de gegeven $\\nu$ en $\\xi$ gaan we de kwantielen bepalen van de cdf's die we in de cel hierboven hebben opgesteld.\n",
    "<break>\n",
    "Het verkrijgen van de kwantielen gaat op een soortgelijke manier als het verkrijgen van de probabilities. Dit gaan we ook bepalen met dummy variables. Ik kijk dan welke cdf's in de vector van vCdf kleiner zijn dan dU. Die krijgen dan een waarde 1 en de rest 0. Uiteindelijk neem ik de som zodat ik weet hoeveel van die cdf's een dummy variable 1 hebben gekregen. Om de index te weten van de laatste cdf met een dummy variable 1 moet ik dus np.sum(vDummy)-1 doen. Uiteindelijk return ik de de bijbehorende gridpoint. Dit doe ik in een for loop voor alle tijdstippen.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InverseCDF(dNu, dXi, vU):\n",
    "    vGridPoints = np.arange(-20,20,0.001)\n",
    "    vCdf=GetCDF(dNu,dXi)\n",
    "\n",
    "    vX=np.zeros(len(vU))\n",
    "    for i in range(len(vU)):\n",
    "        dU = vU[i]\n",
    "        vDummy=vCdf<dU\n",
    "        vDummy=vDummy+np.zeros(len(vDummy))\n",
    "        dIndex=np.sum(vDummy)-1\n",
    "        if int(dIndex)==0:\n",
    "            vX[i]=vGridPoints[0]\n",
    "        \n",
    "        elif int(dIndex) + 1 == len(vGridPoints):\n",
    "            vX[i]= vGridPoints[int(dIndex)]\n",
    "            \n",
    "        else:\n",
    "            vX[i]=vGridPoints[int(dIndex)] +( vGridPoints[int(dIndex)+1] - vGridPoints[int(dIndex)] )  * \\\n",
    "                  (dU - vCdf [int(dIndex)] ) / ( vCdf [int(dIndex)+1] - vCdf [int(dIndex)] ) \n",
    "    return vX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opstellen van de log-likelihood\n",
    "In de cel hieronder stel ik de log-likelihood op. De log-likelihood ziet er als volgt uit:\n",
    "$$\\log(c)+\\log\\left(\\mathbb{K}_{\\frac{d+\\nu}{2}}\\left(\\sqrt{(\\nu+x'\\Sigma^{-1}x)\\gamma'\\Sigma^{-1}\\gamma}\\right)\\right)+x'\\Sigma^{-1}\\gamma + \\frac{d+\\nu}{2}\\log\\left(\\sqrt{(\\nu+x'\\Sigma^{-1}x)\\gamma'\\Sigma^{-1}\\gamma}\\right)-\\frac{d+\\nu}{2}\\log\\left(1+\\frac{x'\\Sigma^{-1}x}{\\nu}\\right)-\\log(f(x_{1t}|\\nu,\\xi_1))-\\log(f(x_{2t}|\\nu,\\xi_2))$$\n",
    "\n",
    "- $\\mathbb{K}_{\\frac{d+\\nu}{2}}(.)$ is de modified Bessel function of the second kind met index $\\frac{d+\\nu}{2}$\n",
    "- x is gedefinieerd als:\n",
    "$$x=\\begin{pmatrix} x_{1t}\\\\ x_{2t}\\end{pmatrix}$$\n",
    "waarbij $x_{it}$ een kwantiel is die verkregen is door de inverse cdf: $x_{it}=Skwt^{-1}_{\\nu,\\xi_i}(u_{it})$ voor i=1,2 en $t=1\\dots n$. Waarbij $u_{it}$ de kansen van de tijdreeksen zijn die verkregen zijn door PIT.\n",
    "\n",
    "- $\\gamma$ is gedefinieerd als:\n",
    "$$\\gamma=\\begin{pmatrix} \\xi_{1}\\\\ \\xi_{2}\\end{pmatrix}$$\n",
    "\n",
    "- In dit geval is $\\Sigma=R$ en $R$ is als volgt gedefinieerd:\n",
    "\n",
    "$$R=\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$$\n",
    "\n",
    "- c is gedefinieerd als:\n",
    "$$c=\\frac{2^{\\frac{2-(\\nu+d)}{2}}}{\\Gamma\\left(\\frac{\\nu}{2}\\right)(\\pi\\nu)^{\\frac{d}{2}}\\left|\\Sigma\\right|^{0.5}} $$\n",
    "\n",
    "- d is de dimensie van de copula. In dit geval 2\n",
    "\n",
    "- $f(x_{it}|\\nu,\\xi_i)$ is de univariate copula met skewed student-t verdeling die gedefinieerd is boven GetCDF en onder deze cel\n",
    "<br>\n",
    "<br>\n",
    "De parameters die we moeten optimaliseren zijn $\\rho,\\nu,\\xi_1,\\xi_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitie van de $f(x_{it}|\\nu,\\xi_i)$\n",
    "Dit is de pdf die nodig is om in de log-likeihood te stoppen. In andere woorden dit is de $f(x_{it}|\\nu,\\xi_i)$ in de cel hierboven. De functie $f(x_{it}|\\nu,\\xi_i)$ is als volgt gedefnieerd:\n",
    "$$f(x|\\nu,\\xi_i)=\\frac{c\\mathbb{K}_{\\frac{1+\\nu}{2}}\\left(\\sqrt{(\\nu+x^2)\\xi_i^2}\\right)\\exp(x\\xi_i)}{\\left(\\sqrt{(\\nu+x^2)\\xi_i^2}\\right)^{-\\frac{1+\\nu}{2}}\\left(1+\\frac{x^2}{\\nu}\\right)^\\frac{1+\\nu}{2}}$$\n",
    "\n",
    "- c is als volgt gedefinieerd:\n",
    "$$c=\\frac{2^{\\frac{2-(\\nu+1)}{2}}}{\\Gamma\\left(\\frac{\\nu}{2}\\right)(\\pi\\nu)^{\\frac{1}{2}}}$$\n",
    "\n",
    "- $\\mathbb{K}_{\\frac{1+\\nu}{2}}(.)$ is de modified Bessel function of second kind\n",
    "<break>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPDF(dX,dNu,dXi):\n",
    "    iDimension = 1\n",
    "    dC = 2**( ( 2- ( dNu+iDimension ) ) / 2) / (gamma(dNu/2) * np.sqrt(np.pi*dNu) )\n",
    "    dSuperscript = np.sqrt((dNu + dX**2)*dXi**2)\n",
    "    dPdf = (dC * kv((dNu+iDimension)/2,dSuperscript) * np.exp(dX*dXi)) / (dSuperscript**(- ( iDimension+dNu )/2 ) \\\n",
    "           * (1+ dX**2/dNu)**((iDimension+dNu)/2))\n",
    "    return dPdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wordt de log-likelihood gedefinieerd die boven is beschreven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Parameters_Copula_Skew(vU1,vU2):\n",
    "    dRho12 = 0.91677754\n",
    "    dNu = 7.40781997\n",
    "    dXi1 = 0.00001\n",
    "    dXi2 = 0.00001\n",
    "    vTheta = np.array([dRho12, dNu, dXi1, dXi2])\n",
    "    \n",
    "    def LL_Copula_Skew(vTheta,vU1,vU2):\n",
    "\n",
    "        dRho12 = vTheta[0]\n",
    "        dNu = vTheta[1]\n",
    "        dXi1 = vTheta[2]\n",
    "        dXi2 = vTheta[3]\n",
    "        vGamma=np.array([[dXi1],[dXi2]])\n",
    "        iT = len(vU1)\n",
    "        #mQuantile= st.t.ppf(mUt, dNu, loc=np.zeros(mUt.shape), scale=np.ones(mUt.shape))\n",
    "        iDimension = 2\n",
    "        mR = np.ones((iDimension,iDimension))\n",
    "        mR[1,0] = dRho12\n",
    "        mR[0,1] = dRho12\n",
    "        mInv_R = inv(mR)\n",
    "        dC = 2**( (2- ( dNu+iDimension ) ) / 2 ) / ( gamma( dNu/2 ) * ( np.pi*dNu )**( iDimension/2 ) * np.sqrt(det(mR)) ) \n",
    "        vX1=InverseCDF(dNu, dXi1, vU1)\n",
    "        vX2=InverseCDF(dNu, dXi2, vU2)\n",
    "        dSum=0\n",
    "        \n",
    "        for t in range(0,iT):\n",
    "            vX = np.zeros( ( iDimension,1 ) )\n",
    "            vX[0] = vX1[t]\n",
    "            vX[1] = vX2[t]\n",
    "            dPdf1 = getPDF(vX[0], dNu, dXi1)\n",
    "            dPdf2 = getPDF(vX[1], dNu, dXi2)\n",
    "            dSuperscript = np.sqrt( (dNu + vX.T @ mInv_R @ vX ) @ vGamma.T @ mInv_R @ vGamma)\n",
    "            dLogLikelihood = np.log(dC)+np.log(kv((dNu+iDimension)/2,dSuperscript)) + (vX.T @ mInv_R @ vGamma) + \\\n",
    "                         (dNu+iDimension)/2 * np.log(dSuperscript) - (dNu+iDimension)/2 * np.log(1+ (vX.T @ mInv_R @ vX)/dNu) \\\n",
    "                         -np.log(dPdf1) - np.log(dPdf2)   \n",
    "            dSum += np.asscalar(dLogLikelihood)\n",
    "        #vAppel.append([vTheta,-dSum])\n",
    "        #print(dLogLikelihood)\n",
    "        #print(vTheta.T)\n",
    "        \n",
    "        return -dSum\n",
    "        \n",
    "    def Optimizer(vU1, vU2, initials, function, bnds):\n",
    "        result = minimize(function, initials, args=(vU1,vU2), \\\n",
    "                          options ={'eps':1e-09, 'disp': True, 'maxiter':200}, method='SLSQP',bounds=bnds)\n",
    "        return result\n",
    "    #vAppel=[]\n",
    "    bounds = ((-0.9999999, 0.9999999),(2.1,50), (-2,2), (-2,2))\n",
    "    result=Optimizer(vU1, vU2, vTheta, LL_Copula_Skew, bounds)\n",
    "    return result.x, -result.fun, result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -1492.514546530313\n",
      "            Iterations: 15\n",
      "            Function evaluations: 122\n",
      "            Gradient evaluations: 15\n",
      "[ 0.92326162  5.14010071 -0.05907519 -0.06048458]\n",
      "1492.514546530313\n",
      "True\n",
      "99.80720170000001\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "(vTheta,LL_Copula_Skew,success_Copula_Skew)=Parameters_Copula_Skew(vU1,vU2)\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "print(vTheta)\n",
    "print(LL_Copula_Skew)\n",
    "print(success_Copula_Skew)\n",
    "print(execution_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
